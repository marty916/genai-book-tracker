{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation/sample_dashboard.ipynb\n",
    "\n",
    "# ---\n",
    "# Jupyter Notebook: Book Tracker - Phoenix Evaluation & Visualization\n",
    "# ---\n",
    "\n",
    "# 1. Load Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load test set and (optionally) model responses if you saved them to a CSV\n",
    "df = pd.read_csv(\"test_set.csv\")\n",
    "# If you also saved model outputs (recommended!), load:\n",
    "# results_df = pd.read_csv(\"llm_results.csv\")\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# 2. (Optional) Collect LLM Responses\n",
    "\n",
    "# For demo, we'll fake LLM responses here (replace with your actual LLM outputs!)\n",
    "df['llm_response'] = [\n",
    "    \"The Garden Primer by Barbara Damrosch\",\n",
    "    \"Dune by Frank Herbert\",\n",
    "    \"No real books found. Suggest 'The Quantum World' by Kenneth Ford for quantum science.\",\n",
    "    \"No real books found. Suggest 'The Art of Yoga' for human yoga instead.\",\n",
    "    \"Salt, Fat, Acid, Heat by Samin Nosrat\",\n",
    "    \"No real books found. Suggest 'How to Train Your Dragon' (fiction) for entertainment.\"\n",
    "]\n",
    "\n",
    "display(df)\n",
    "\n",
    "# 3. Simple Manual Comparison\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"Prompt: {row['question']}\")\n",
    "    print(f\"Reference: {row['reference_answer']}\")\n",
    "    print(f\"LLM Response: {row['llm_response']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# 4. (Optional) Semantic Similarity Using OpenAI Embeddings\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    result = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return result.data[0].embedding\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Calculate similarity for each row\n",
    "sims = []\n",
    "for idx, row in df.iterrows():\n",
    "    ref_emb = get_embedding(row['reference_answer'])\n",
    "    llm_emb = get_embedding(row['llm_response'])\n",
    "    sim = cosine_similarity(ref_emb, llm_emb)\n",
    "    sims.append(sim)\n",
    "\n",
    "df['similarity'] = sims\n",
    "display(df[['question', 'reference_answer', 'llm_response', 'similarity']])\n",
    "\n",
    "# 5. Simple Pass/Fail Visualization\n",
    "\n",
    "threshold = 0.80\n",
    "df['pass'] = df['similarity'] >= threshold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "df['pass'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('LLM Output Evaluation: Pass/Fail')\n",
    "plt.xlabel('Pass')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Fail', 'Pass'], rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# 6. (Optional) Phoenix Observability Integration\n",
    "\n",
    "# If you have the Phoenix Python client, you can add a cell here to pull traces or visualize results from your Phoenix Cloud project!\n",
    "# See: https://docs.arize.com/phoenix/\n",
    "\n",
    "print(\"Extend this notebook with Phoenix dashboard integration as needed!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
